{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SantiagoGomezfpv/hyperparameter/blob/main/pytorchModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWgo41h7koFX"
      },
      "source": [
        "# Modelo Fully Connected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZn7ia9Mkm2I",
        "outputId": "009266ad-e778-4bdc-aae9-8325a2db3109"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 33858455.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 917722.46it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:49<00:00, 33398.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3823104.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "[1,   200] loss: 0.663\n",
            "[1,   400] loss: 0.318\n",
            "[1,   600] loss: 0.269\n",
            "[1,   800] loss: 0.242\n",
            "[2,   200] loss: 0.181\n",
            "[2,   400] loss: 0.166\n",
            "[2,   600] loss: 0.149\n",
            "[2,   800] loss: 0.155\n",
            "[3,   200] loss: 0.111\n",
            "[3,   400] loss: 0.121\n",
            "[3,   600] loss: 0.105\n",
            "[3,   800] loss: 0.107\n",
            "[4,   200] loss: 0.080\n",
            "[4,   400] loss: 0.083\n",
            "[4,   600] loss: 0.086\n",
            "[4,   800] loss: 0.086\n",
            "[5,   200] loss: 0.063\n",
            "[5,   400] loss: 0.066\n",
            "[5,   600] loss: 0.068\n",
            "[5,   800] loss: 0.062\n",
            "[6,   200] loss: 0.052\n",
            "[6,   400] loss: 0.053\n",
            "[6,   600] loss: 0.050\n",
            "[6,   800] loss: 0.054\n",
            "[7,   200] loss: 0.042\n",
            "[7,   400] loss: 0.044\n",
            "[7,   600] loss: 0.041\n",
            "[7,   800] loss: 0.042\n",
            "[8,   200] loss: 0.035\n",
            "[8,   400] loss: 0.031\n",
            "[8,   600] loss: 0.033\n",
            "[8,   800] loss: 0.036\n",
            "[9,   200] loss: 0.028\n",
            "[9,   400] loss: 0.026\n",
            "[9,   600] loss: 0.029\n",
            "[9,   800] loss: 0.035\n",
            "[10,   200] loss: 0.025\n",
            "[10,   400] loss: 0.023\n",
            "[10,   600] loss: 0.023\n",
            "[10,   800] loss: 0.025\n",
            "Precisión del modelo en el conjunto de test: 97 %\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Cargar y preparar datos MNIST\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Definir la red neuronal fully connected\n",
        "class FullyConnectedNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Crear modelo y optimizador\n",
        "model = FullyConnectedNet()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Entrenar el modelo\n",
        "for epoch in range(10):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:\n",
        "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 200))\n",
        "            running_loss = 0.0\n",
        "\n",
        "# Evaluar el modelo\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Precisión del modelo en el conjunto de test: %d %%' % (100 * correct / total))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHRdE_06k_wa"
      },
      "source": [
        "# Modelo CNN\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_w6MPqstlBKY",
        "outputId": "9e8a626a-45f8-4a1c-951c-5e4722590773"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,   100] loss: 0.693\n",
            "[1,   200] loss: 0.179\n",
            "[1,   300] loss: 0.120\n",
            "[1,   400] loss: 0.108\n",
            "[1,   500] loss: 0.094\n",
            "[1,   600] loss: 0.087\n",
            "[1,   700] loss: 0.071\n",
            "[1,   800] loss: 0.061\n",
            "[1,   900] loss: 0.062\n",
            "[2,   100] loss: 0.062\n",
            "[2,   200] loss: 0.047\n",
            "[2,   300] loss: 0.045\n",
            "[2,   400] loss: 0.057\n",
            "[2,   500] loss: 0.044\n",
            "[2,   600] loss: 0.049\n",
            "[2,   700] loss: 0.040\n",
            "[2,   800] loss: 0.040\n",
            "[2,   900] loss: 0.041\n",
            "[3,   100] loss: 0.032\n",
            "[3,   200] loss: 0.030\n",
            "[3,   300] loss: 0.028\n",
            "[3,   400] loss: 0.035\n",
            "[3,   500] loss: 0.030\n",
            "[3,   600] loss: 0.027\n",
            "[3,   700] loss: 0.029\n",
            "[3,   800] loss: 0.035\n",
            "[3,   900] loss: 0.031\n",
            "[4,   100] loss: 0.024\n",
            "[4,   200] loss: 0.021\n",
            "[4,   300] loss: 0.018\n",
            "[4,   400] loss: 0.024\n",
            "[4,   500] loss: 0.021\n",
            "[4,   600] loss: 0.027\n",
            "[4,   700] loss: 0.021\n",
            "[4,   800] loss: 0.033\n",
            "[4,   900] loss: 0.026\n",
            "[5,   100] loss: 0.017\n",
            "[5,   200] loss: 0.018\n",
            "[5,   300] loss: 0.015\n",
            "[5,   400] loss: 0.019\n",
            "[5,   500] loss: 0.025\n",
            "[5,   600] loss: 0.023\n",
            "[5,   700] loss: 0.012\n",
            "[5,   800] loss: 0.023\n",
            "[5,   900] loss: 0.020\n",
            "Finished Training\n",
            "Accuracy on the 10000 test images: 99 %\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Definir la arquitectura de la CNN\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.fc1 = nn.Linear(64*5*5, 128)  # Corregido el tamaño de entrada\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = torch.max_pool2d(x, 2)\n",
        "        x = self.conv2(x)\n",
        "        x = torch.relu(x)\n",
        "        x = torch.max_pool2d(x, 2)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Cargar los datos de MNIST\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "trainset, valset = torch.utils.data.random_split(trainset, [54000, 6000])\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Instanciar el modelo y definir la función de pérdida y el optimizador\n",
        "model = CNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 100))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# Evaluación del modelo\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_-EDpbmlxa4"
      },
      "source": [
        "# Optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPkwrCiQYM5M",
        "outputId": "4fd37c3f-0d09-4649-e25b-a0bf0da44fd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.6.1-py3-none-any.whl (380 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.29)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.3-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.11.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
            "Installing collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.3 alembic-1.13.1 colorlog-6.8.2 optuna-3.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8X09lvPW7C0",
        "outputId": "fc4c138f-122e-4deb-f3db-a9bc6f12bac0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-05-07 12:55:40,802] A new study created in memory with name: no-name-7d27141b-baaa-4b34-9ae4-7e592bc3e539\n",
            "[I 2024-05-07 12:58:51,877] Trial 0 finished with value: 0.9868333333333333 and parameters: {'n_filters1': 23, 'n_filters2': 55, 'n_units': 224, 'lr': 0.000998756876510727}. Best is trial 0 with value: 0.9868333333333333.\n",
            "[I 2024-05-07 13:03:04,321] Trial 1 finished with value: 0.9873333333333333 and parameters: {'n_filters1': 29, 'n_filters2': 75, 'n_units': 142, 'lr': 0.0014745557307656977}. Best is trial 1 with value: 0.9873333333333333.\n",
            "[I 2024-05-07 13:10:23,019] Trial 2 finished with value: 0.9893333333333333 and parameters: {'n_filters1': 63, 'n_filters2': 62, 'n_units': 184, 'lr': 0.0005051031785180668}. Best is trial 2 with value: 0.9893333333333333.\n",
            "[I 2024-05-07 13:16:18,739] Trial 3 finished with value: 0.9795 and parameters: {'n_filters1': 43, 'n_filters2': 93, 'n_units': 172, 'lr': 4.786286481227014e-05}. Best is trial 2 with value: 0.9893333333333333.\n",
            "[I 2024-05-07 13:22:17,561] Trial 4 finished with value: 0.9746666666666667 and parameters: {'n_filters1': 52, 'n_filters2': 79, 'n_units': 88, 'lr': 0.013207311006328364}. Best is trial 2 with value: 0.9893333333333333.\n",
            "[I 2024-05-07 13:29:09,023] Trial 5 finished with value: 0.967 and parameters: {'n_filters1': 55, 'n_filters2': 85, 'n_units': 243, 'lr': 0.012696017406096922}. Best is trial 2 with value: 0.9893333333333333.\n",
            "[I 2024-05-07 13:33:39,574] Trial 6 finished with value: 0.9865 and parameters: {'n_filters1': 45, 'n_filters2': 50, 'n_units': 112, 'lr': 0.001887961702198018}. Best is trial 2 with value: 0.9893333333333333.\n",
            "[I 2024-05-07 13:39:15,673] Trial 7 finished with value: 0.108 and parameters: {'n_filters1': 56, 'n_filters2': 63, 'n_units': 90, 'lr': 0.007924317492650585}. Best is trial 2 with value: 0.9893333333333333.\n",
            "[I 2024-05-07 13:44:03,896] Trial 8 finished with value: 0.9891666666666666 and parameters: {'n_filters1': 36, 'n_filters2': 86, 'n_units': 78, 'lr': 0.0015513858568748408}. Best is trial 2 with value: 0.9893333333333333.\n",
            "[I 2024-05-07 13:51:35,296] Trial 9 finished with value: 0.108 and parameters: {'n_filters1': 61, 'n_filters2': 90, 'n_units': 244, 'lr': 0.06389971541793062}. Best is trial 2 with value: 0.9893333333333333.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Número de pruebas finalizadas: 10\n",
            "Mejor prueba:\n",
            "  Valor: 0.9893333333333333\n",
            "  Parámetros: \n",
            "    n_filters1: 63\n",
            "    n_filters2: 62\n",
            "    n_units: 184\n",
            "    lr: 0.0005051031785180668\n"
          ]
        }
      ],
      "source": [
        "import torch                                        # Librería para aprendizaje profundo\n",
        "import torch.nn as nn                               # Contiene módulos para construir redes neuronales\n",
        "import torch.optim as optim                         # Contiene optimizadores para entrenar redes neuronales\n",
        "import torchvision                                  # Librería para datasets y transformaciones de visión computacional\n",
        "import torchvision.transforms as transforms         # Contiene funciones para preprocesar imágenes\n",
        "import optuna                                       # Librería para optimización hiperparámetros\n",
        "\n",
        "# Define la arquitectura de la CNN (Red Neuronal Convolucional)\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, trial):\n",
        "        super(CNN, self).__init__()\n",
        "        # Capa de convolución 1 con número de filtros variable\n",
        "        self.conv1 = nn.Conv2d(1, trial.suggest_int('n_filters1', 16, 64), kernel_size=3, stride=1)\n",
        "        # Capa de convolución 2 con número de filtros variable, dependiendo de la salida de la capa anterior\n",
        "        self.conv2 = nn.Conv2d(trial.suggest_int('n_filters1', 16, 64), trial.suggest_int('n_filters2', 32, 128), kernel_size=3, stride=1)\n",
        "        # Capa totalmente conectada (fully connected) con número de unidades variable\n",
        "        self.fc1 = nn.Linear(trial.suggest_int('n_filters2', 32, 128) * 5 * 5, trial.suggest_int('n_units', 64, 256))\n",
        "        # Capa de salida con 10 unidades para clasificar en las 10 clases de MNIST\n",
        "        self.fc2 = nn.Linear(trial.suggest_int('n_units', 64, 256), 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Operaciones en la propagación hacia adelante\n",
        "        x = self.conv1(x)\n",
        "        x = torch.relu(x)  # Función de activación ReLU (introducir no linealidades en los datos)\n",
        "        x = torch.max_pool2d(x, 2)  # Max pooling con tamaño de kernel 2x2\n",
        "        x = self.conv2(x)\n",
        "        x = torch.relu(x)\n",
        "        x = torch.max_pool2d(x, 2)\n",
        "        x = torch.flatten(x, 1)  # Aplanar la salida para la capa totalmente conectada\n",
        "        x = self.fc1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Cargar el conjunto de datos MNIST\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convertir las imágenes a tensores\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalizar los valores de píxeles\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "# Dividir el conjunto de datos de entrenamiento en entrenamiento y validación\n",
        "trainset, valset = torch.utils.data.random_split(trainset, [54000, 6000])\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Definir la función objetivo para Optuna\n",
        "def objective(trial):\n",
        "    model = CNN(trial)\n",
        "    criterion = nn.CrossEntropyLoss()  # Función de pérdida para problemas de clasificación\n",
        "    optimizer = optim.Adam(model.parameters(), lr=trial.suggest_float('lr', 1e-5, 1e-1, log=True))  # Optimizador Adam con tasa de aprendizaje variable\n",
        "    num_epochs = 5\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()               # Reiniciar los gradientes acumulados\n",
        "            outputs = model(inputs)             # Propagación hacia adelante\n",
        "            loss = criterion(outputs, labels)   # Calcular la pérdida\n",
        "            loss.backward()                     # Retropropagación\n",
        "            optimizer.step()                    # Actualización de los pesos\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "    # Evaluación del modelo en el conjunto de validación\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in valloader:\n",
        "            images, labels = data\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)  # Obtener las predicciones\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Ejecutar la optimización de Optuna\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "print('Número de pruebas finalizadas:', len(study.trials))\n",
        "print('Mejor prueba:')\n",
        "trial = study.best_trial\n",
        "print('  Valor: {}'.format(trial.value))\n",
        "print('  Parámetros: ')\n",
        "for key, value in trial.params.items():\n",
        "    print('    {}: {}'.format(key, value))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTn7sbd1YHUg"
      },
      "source": [
        "# Optimizador Optuna Elena\n",
        "\n",
        "[Optuna-optimization-for-PyTorch-CNN](https://github.com/elena-ecn/optuna-optimization-for-PyTorch-CNN/blob/main/optuna_optimization.py#L179)\n",
        "\n",
        "El conjunto de datos MNIST contiene **60.000** imágenes de **entrenamiento** y **10.000** imágenes de **prueba**.\n",
        "donde cada muestra es una imagen pequeña, cuadrada, en escala de grises de **28 × 28** píxeles de\n",
        "dígitos únicos escritos a mano entre 0 y 9.\n",
        "\n",
        "Los hiperparámetros de CNN elegidos para optimizar son:\n",
        "\n",
        "\n",
        "\n",
        "*   Tipo de optimizador\n",
        "*   Tasa de aprendizaje\n",
        "*   Valores de abandono\n",
        "*   Número de capas convolucionales\n",
        "*   Número de filtros de capas convolucionales\n",
        "*   Número de neuronas de capas completamente conectadas.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4qqlP5VYrGV"
      },
      "outputs": [],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2G7m3pOnq8ka"
      },
      "outputs": [],
      "source": [
        "import os                               # Importa el módulo os para interactuar con el sistema operativo\n",
        "import torch                            # Importa PyTorch\n",
        "import torchvision                      # Importa torchvision para trabajar con conjuntos de datos y modelos preentrenados\n",
        "import torch.nn as nn                   # Importa el módulo nn (neural network) de PyTorch para definir redes neuronales\n",
        "import torch.nn.functional as F         # Importa funciones de activación y pérdida de PyTorch\n",
        "import torch.optim as optim             # Importa optimizadores de PyTorch\n",
        "import optuna                           # Importa Optuna para la optimización de hiperparámetros\n",
        "from optuna.trial import TrialState     # Importa TrialState para el estado de las pruebas en Optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uP1oLxX3Yrsr"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, trial, num_conv_layers, num_filters, num_neurons, drop_conv2, drop_fc1):\n",
        "        \"\"\"\n",
        "        Parámetros:\n",
        "            - trial (optuna.trial._trial.Trial): Prueba de Optuna\n",
        "            - num_conv_layers (int):             Número de capas convolucionales\n",
        "            - num_filters (list):                Número de filtros de las capas convolucionales\n",
        "            - num_neurons (int):                 Número de neuronas de las capas totalmente conectadas\n",
        "            - drop_conv2 (float):                Ratio de dropout para la capa convolucional 2\n",
        "            - drop_fc1 (float):                  Ratio de dropout para FC1\n",
        "        \"\"\"\n",
        "        super(Net, self).__init__()                                                     # Inicializa la clase padre\n",
        "        in_size = 28                                                                    # Tamaño de la imagen de entrada (28 píxeles)\n",
        "        kernel_size = 3                                                                 # Tamaño del filtro de convolución\n",
        "\n",
        "        # Define las capas convolucionales\n",
        "        self.convs = nn.ModuleList([nn.Conv2d(1, num_filters[0], kernel_size=(3, 3))])  # Lista con las capas convolucionales\n",
        "        out_size = in_size - kernel_size + 1                                            # Tamaño del kernel de salida\n",
        "        out_size = int(out_size / 2)                                                    # Tamaño después del max-pooling\n",
        "        for i in range(1, num_conv_layers):\n",
        "            self.convs.append(nn.Conv2d(in_channels=num_filters[i-1], out_channels=num_filters[i], kernel_size=(3, 3)))\n",
        "            out_size = out_size - kernel_size + 1                                       # Tamaño del kernel de salida\n",
        "            out_size = int(out_size/2)                                                  # Tamaño después del max-pooling\n",
        "\n",
        "        self.conv2_drop = nn.Dropout2d(p=drop_conv2)                                    # Dropout para conv2\n",
        "        self.out_feature = num_filters[num_conv_layers-1] * out_size * out_size         # Tamaño de las características aplanadas\n",
        "        self.fc1 = nn.Linear(self.out_feature, num_neurons)                             # Capa totalmente conectada 1\n",
        "        self.fc2 = nn.Linear(num_neurons, 10)                                           # Capa totalmente conectada 2\n",
        "        self.p1 = drop_fc1                                                              # Ratio de dropout para FC1\n",
        "\n",
        "        # Inicializa los pesos con la inicialización de He\n",
        "        for i in range(1, num_conv_layers):\n",
        "            nn.init.kaiming_normal_(self.convs[i].weight, nonlinearity='relu')\n",
        "            if self.convs[i].bias is not None:\n",
        "                nn.init.constant_(self.convs[i].bias, 0)\n",
        "        nn.init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Propagación hacia adelante.\n",
        "\n",
        "        Parámetros:\n",
        "            - x (torch.Tensor): Tensor de entrada de tamaño [N,1,28,28]\n",
        "        Devuelve:\n",
        "            - (torch.Tensor): El tensor de salida después de la propagación hacia adelante [N,10]\n",
        "        \"\"\"\n",
        "        for i, conv_i in enumerate(self.convs):  # Para cada capa convolucional\n",
        "            if i == 2:  # Añade dropout si es la capa 2\n",
        "                x = F.relu(F.max_pool2d(self.conv2_drop(conv_i(x)), 2))  # Conv_i, dropout, max-pooling, RelU\n",
        "            else:\n",
        "                x = F.relu(F.max_pool2d(conv_i(x), 2))                   # Conv_i, max-pooling, RelU\n",
        "\n",
        "        x = x.view(-1, self.out_feature)                                 # Aplana el tensor\n",
        "        x = F.relu(self.fc1(x))                                          # FC1, RelU\n",
        "        x = F.dropout(x, p=self.p1, training=self.training)              # Aplica dropout después de FC1 solo durante el entrenamiento\n",
        "        x = self.fc2(x)                                                  # FC2\n",
        "\n",
        "        return F.log_softmax(x, dim=1)                                   # log(softmax(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OXbprGCgjcL"
      },
      "outputs": [],
      "source": [
        "def train(network, optimizer):\n",
        "    \"\"\"\n",
        "    Entrena el modelo.\n",
        "\n",
        "    Parámetros:\n",
        "        - red (__main__.Net):                      La CNN\n",
        "        - optimizador (torch.optim.<optimizador>): El optimizador para la CNN\n",
        "    \"\"\"\n",
        "    network.train()  # Establece el módulo en modo de entrenamiento (afecta solo ciertos módulos)\n",
        "    for batch_i, (data, target) in enumerate(train_loader):  # Para cada lote\n",
        "\n",
        "        # Limita los datos de entrenamiento para una computación más rápida\n",
        "        if batch_i * batch_size_train > number_of_train_examples:\n",
        "            break\n",
        "\n",
        "        optimizer.zero_grad()                                 # Borra los gradientes\n",
        "        output = network(data.to(device))                     # Propagación hacia adelante\n",
        "        loss = F.nll_loss(output, target.to(device))          # Calcula la pérdida (negative log likelihood: −log(y))\n",
        "        loss.backward()                                       # Calcula los gradientes\n",
        "        optimizer.step()                                      # Actualiza los pesos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vFp2NDzg1fN"
      },
      "outputs": [],
      "source": [
        "def test(network):\n",
        "    \"\"\"\n",
        "    Evalúa el modelo.\n",
        "\n",
        "    Parámetros:\n",
        "        - red (__main__.Net): La CNN\n",
        "\n",
        "    Devuelve:\n",
        "        - accuracy_test (torch.Tensor): La precisión de la prueba\n",
        "    \"\"\"\n",
        "    network.eval()         # Establece el módulo en modo de evaluación (afecta solo ciertos módulos)\n",
        "    correct = 0\n",
        "    with torch.no_grad():  # Deshabilita el cálculo de gradientes (cuando estás seguro de que no llamarás a Tensor.backward())\n",
        "        for batch_i, (data, target) in enumerate(test_loader):  # Para cada lote\n",
        "\n",
        "            # Limita los datos de prueba para una computación más rápida\n",
        "            if batch_i * batch_size_test > number_of_test_examples:\n",
        "                break\n",
        "\n",
        "            output = network(data.to(device))                                     # Propagación hacia adelante\n",
        "            pred = output.data.max(1, keepdim=True)[1]                            # Encuentra el valor máximo en cada fila, devuelve los índices de los valores máximos\n",
        "            correct += pred.eq(target.to(device).data.view_as(pred)).sum()        # Calcula las predicciones correctas\n",
        "\n",
        "    accuracy_test = correct / len(test_loader.dataset)\n",
        "\n",
        "    return accuracy_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glgDHgrnAqFT"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    \"\"\"Función objetivo a ser optimizada por Optuna.\n",
        "\n",
        "    Hiperparámetros elegidos para ser optimizados: optimizador, tasa de aprendizaje,\n",
        "    valores de dropout, número de capas convolucionales, número de filtros de las\n",
        "    capas convolucionales, número de neuronas de las capas totalmente conectadas.\n",
        "\n",
        "    Entradas:\n",
        "        - trial (optuna.trial._trial.Trial): Prueba de Optuna\n",
        "    Devuelve:\n",
        "        - accuracy (torch.Tensor): La precisión de la prueba. Parámetro a maximizar.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define el rango de valores a probar para los hiperparámetros\n",
        "    num_conv_layers = trial.suggest_int(\"num_conv_layers\", 2, 3)  # Número de capas convolucionales\n",
        "    num_filters = [int(trial.suggest_discrete_uniform(\"num_filter_\"+str(i), 16, 128, 16))\n",
        "                   for i in range(num_conv_layers)]               # Número de filtros para las capas convolucionales\n",
        "    num_neurons = trial.suggest_int(\"num_neurons\", 10, 400, 10)   # Número de neuronas de la capa FC1\n",
        "    drop_conv2 = trial.suggest_float(\"drop_conv2\", 0.2, 0.5)      # Dropout para la capa convolucional 2\n",
        "    drop_fc1 = trial.suggest_float(\"drop_fc1\", 0.2, 0.5)          # Dropout para la capa FC1\n",
        "\n",
        "    # Genera el modelo\n",
        "    model = Net(trial, num_conv_layers, num_filters, num_neurons, drop_conv2,  drop_fc1).to(device)\n",
        "\n",
        "    # Genera los optimizadores\n",
        "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])  # Optimizadores\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)                                 # Tasas de aprendizaje\n",
        "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
        "\n",
        "    # Entrenamiento del modelo\n",
        "    for epoch in range(n_epochs):\n",
        "        train(model, optimizer)  # Entrena el modelo\n",
        "        accuracy = test(model)   # Evalúa el modelo\n",
        "\n",
        "        # Para la poda (detiene la prueba temprano si no es prometedora)\n",
        "        trial.report(accuracy, epoch)\n",
        "        # Maneja la poda basada en el valor intermedio.\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHwEWLshA2Z5",
        "outputId": "9d530b92-d1d4-409e-c130-a57ae72ad80c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to /files/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:01<00:00, 5444301.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /files/MNIST/raw/train-images-idx3-ubyte.gz to /files/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to /files/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 160017.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /files/MNIST/raw/train-labels-idx1-ubyte.gz to /files/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to /files/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:04<00:00, 393172.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /files/MNIST/raw/t10k-images-idx3-ubyte.gz to /files/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to /files/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4292593.23it/s]\n",
            "[I 2024-05-07 17:09:26,506] A new study created in memory with name: no-name-123ea2a7-cc3a-482b-b37c-b19f3f9d1e36\n",
            "<ipython-input-7-67589e027cdb>:16: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
            "  num_filters = [int(trial.suggest_discrete_uniform(\"num_filter_\"+str(i), 16, 128, 16))\n",
            "<ipython-input-7-67589e027cdb>:18: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  num_neurons = trial.suggest_int(\"num_neurons\", 10, 400, 10)  # Número de neuronas de la capa FC1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /files/MNIST/raw/t10k-labels-idx1-ubyte.gz to /files/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-05-07 17:27:21,348] Trial 0 finished with value: 0.48919999599456787 and parameters: {'num_conv_layers': 2, 'num_filter_0': 128.0, 'num_filter_1': 128.0, 'num_neurons': 340, 'drop_conv2': 0.21439508970072876, 'drop_fc1': 0.409350471900658, 'optimizer': 'SGD', 'lr': 4.735407160810673e-05}. Best is trial 0 with value: 0.48919999599456787.\n",
            "[I 2024-05-07 17:37:19,810] Trial 1 finished with value: 0.595300018787384 and parameters: {'num_conv_layers': 2, 'num_filter_0': 112.0, 'num_filter_1': 48.0, 'num_neurons': 360, 'drop_conv2': 0.3637459814288115, 'drop_fc1': 0.36470795939010936, 'optimizer': 'Adam', 'lr': 0.0008584929846386581}. Best is trial 1 with value: 0.595300018787384.\n",
            "[I 2024-05-07 17:45:10,827] Trial 2 finished with value: 0.5598999857902527 and parameters: {'num_conv_layers': 3, 'num_filter_0': 80.0, 'num_filter_1': 48.0, 'num_filter_2': 112.0, 'num_neurons': 380, 'drop_conv2': 0.29614793864676914, 'drop_fc1': 0.3769548775900181, 'optimizer': 'Adam', 'lr': 1.420577204735963e-05}. Best is trial 1 with value: 0.595300018787384.\n",
            "[I 2024-05-07 17:47:49,748] Trial 3 finished with value: 0.5914999842643738 and parameters: {'num_conv_layers': 2, 'num_filter_0': 16.0, 'num_filter_1': 32.0, 'num_neurons': 290, 'drop_conv2': 0.36518923835495387, 'drop_fc1': 0.2534757834127089, 'optimizer': 'RMSprop', 'lr': 0.006895583476179975}. Best is trial 1 with value: 0.595300018787384.\n",
            "[I 2024-05-07 17:59:50,412] Trial 4 finished with value: 0.5932000279426575 and parameters: {'num_conv_layers': 2, 'num_filter_0': 128.0, 'num_filter_1': 64.0, 'num_neurons': 320, 'drop_conv2': 0.4330365829275203, 'drop_fc1': 0.27775098798956704, 'optimizer': 'RMSprop', 'lr': 7.32614777609411e-05}. Best is trial 1 with value: 0.595300018787384.\n",
            "[I 2024-05-07 18:00:24,749] Trial 5 pruned. \n",
            "[I 2024-05-07 18:01:31,341] Trial 6 pruned. \n",
            "[I 2024-05-07 18:02:49,646] Trial 7 pruned. \n",
            "[I 2024-05-07 18:03:28,431] Trial 8 pruned. \n",
            "[I 2024-05-07 18:04:19,647] Trial 9 pruned. \n",
            "[I 2024-05-07 18:16:06,705] Trial 10 finished with value: 0.5943999886512756 and parameters: {'num_conv_layers': 2, 'num_filter_0': 96.0, 'num_filter_1': 96.0, 'num_neurons': 400, 'drop_conv2': 0.4778284508913528, 'drop_fc1': 0.21227017964763953, 'optimizer': 'Adam', 'lr': 0.0006682581848924286}. Best is trial 1 with value: 0.595300018787384.\n",
            "[I 2024-05-07 18:27:55,017] Trial 11 finished with value: 0.5954999923706055 and parameters: {'num_conv_layers': 2, 'num_filter_0': 96.0, 'num_filter_1': 96.0, 'num_neurons': 400, 'drop_conv2': 0.49920681816314133, 'drop_fc1': 0.2318516934662937, 'optimizer': 'Adam', 'lr': 0.0006600398024524423}. Best is trial 11 with value: 0.5954999923706055.\n",
            "[I 2024-05-07 18:40:10,179] Trial 12 finished with value: 0.5946999788284302 and parameters: {'num_conv_layers': 2, 'num_filter_0': 112.0, 'num_filter_1': 80.0, 'num_neurons': 400, 'drop_conv2': 0.42841445462391153, 'drop_fc1': 0.22108540758700376, 'optimizer': 'Adam', 'lr': 0.0005664536093733146}. Best is trial 11 with value: 0.5954999923706055.\n",
            "[I 2024-05-07 18:46:34,514] Trial 13 finished with value: 0.5939000248908997 and parameters: {'num_conv_layers': 2, 'num_filter_0': 48.0, 'num_filter_1': 80.0, 'num_neurons': 240, 'drop_conv2': 0.4898321347980872, 'drop_fc1': 0.323228450980815, 'optimizer': 'Adam', 'lr': 0.0020965370567537197}. Best is trial 11 with value: 0.5954999923706055.\n",
            "[I 2024-05-07 18:47:42,558] Trial 14 pruned. \n",
            "[I 2024-05-07 18:58:34,476] Trial 15 finished with value: 0.5946000218391418 and parameters: {'num_conv_layers': 2, 'num_filter_0': 112.0, 'num_filter_1': 64.0, 'num_neurons': 340, 'drop_conv2': 0.2777523005114743, 'drop_fc1': 0.44620752002478015, 'optimizer': 'RMSprop', 'lr': 0.0001984886478101404}. Best is trial 11 with value: 0.5954999923706055.\n",
            "[I 2024-05-07 18:59:04,261] Trial 16 pruned. \n",
            "[I 2024-05-07 19:00:01,039] Trial 17 pruned. \n",
            "[I 2024-05-07 19:01:09,483] Trial 18 pruned. \n",
            "[I 2024-05-07 19:01:48,725] Trial 19 pruned. \n",
            "[I 2024-05-07 19:03:00,832] Trial 20 pruned. \n",
            "[I 2024-05-07 19:15:20,294] Trial 21 finished with value: 0.5950000286102295 and parameters: {'num_conv_layers': 2, 'num_filter_0': 112.0, 'num_filter_1': 80.0, 'num_neurons': 400, 'drop_conv2': 0.4482861961736407, 'drop_fc1': 0.22034278775028793, 'optimizer': 'Adam', 'lr': 0.0006796373440228386}. Best is trial 11 with value: 0.5954999923706055.\n",
            "[I 2024-05-07 19:21:12,486] Trial 22 pruned. \n",
            "[I 2024-05-07 19:22:09,946] Trial 23 pruned. \n",
            "[I 2024-05-07 19:23:52,992] Trial 24 pruned. \n",
            "[I 2024-05-07 19:29:14,211] Trial 25 pruned. \n",
            "[I 2024-05-07 19:31:07,542] Trial 26 pruned. \n",
            "[I 2024-05-07 19:31:52,399] Trial 27 pruned. \n",
            "[I 2024-05-07 19:45:00,214] Trial 28 finished with value: 0.5957000255584717 and parameters: {'num_conv_layers': 2, 'num_filter_0': 128.0, 'num_filter_1': 64.0, 'num_neurons': 400, 'drop_conv2': 0.4142984520518183, 'drop_fc1': 0.4117088919540175, 'optimizer': 'RMSprop', 'lr': 0.0002532809844335816}. Best is trial 28 with value: 0.5957000255584717.\n",
            "[I 2024-05-07 19:46:19,267] Trial 29 pruned. \n",
            "[I 2024-05-07 19:47:42,100] Trial 30 pruned. \n",
            "[I 2024-05-07 19:48:48,042] Trial 31 pruned. \n",
            "[I 2024-05-07 19:51:56,461] Trial 32 pruned. \n",
            "[I 2024-05-07 19:52:56,625] Trial 33 pruned. \n",
            "[I 2024-05-07 19:56:15,183] Trial 34 pruned. \n",
            "[I 2024-05-07 19:57:07,500] Trial 35 pruned. \n",
            "[I 2024-05-07 19:58:26,585] Trial 36 pruned. \n",
            "[I 2024-05-07 19:58:42,372] Trial 37 pruned. \n",
            "[I 2024-05-07 19:59:39,099] Trial 38 pruned. \n",
            "[I 2024-05-07 20:00:14,433] Trial 39 pruned. \n",
            "[I 2024-05-07 20:01:15,708] Trial 40 pruned. \n",
            "[I 2024-05-07 20:06:11,890] Trial 41 pruned. \n",
            "[I 2024-05-07 20:17:10,228] Trial 42 finished with value: 0.5946000218391418 and parameters: {'num_conv_layers': 2, 'num_filter_0': 112.0, 'num_filter_1': 64.0, 'num_neurons': 240, 'drop_conv2': 0.42473031112124926, 'drop_fc1': 0.24108877799453443, 'optimizer': 'Adam', 'lr': 0.00052638566713028}. Best is trial 28 with value: 0.5957000255584717.\n",
            "[I 2024-05-07 20:18:16,055] Trial 43 pruned. \n",
            "[I 2024-05-07 20:19:46,547] Trial 44 pruned. \n",
            "[I 2024-05-07 20:30:33,986] Trial 45 finished with value: 0.5931000113487244 and parameters: {'num_conv_layers': 2, 'num_filter_0': 112.0, 'num_filter_1': 64.0, 'num_neurons': 120, 'drop_conv2': 0.29714847361429947, 'drop_fc1': 0.426152512015162, 'optimizer': 'Adam', 'lr': 0.0024244686667653187}. Best is trial 28 with value: 0.5957000255584717.\n",
            "[I 2024-05-07 20:31:47,778] Trial 46 pruned. \n",
            "[I 2024-05-07 20:36:30,550] Trial 47 pruned. \n",
            "[I 2024-05-07 20:37:39,039] Trial 48 pruned. \n",
            "[I 2024-05-07 20:38:45,981] Trial 49 pruned. \n",
            "[I 2024-05-07 20:39:47,253] Trial 50 pruned. \n",
            "[I 2024-05-07 20:50:45,122] Trial 51 finished with value: 0.5946999788284302 and parameters: {'num_conv_layers': 2, 'num_filter_0': 112.0, 'num_filter_1': 64.0, 'num_neurons': 340, 'drop_conv2': 0.2653288731453715, 'drop_fc1': 0.45968073418590116, 'optimizer': 'RMSprop', 'lr': 0.0005518808437566115}. Best is trial 28 with value: 0.5957000255584717.\n",
            "[I 2024-05-07 20:52:44,770] Trial 52 pruned. \n",
            "[I 2024-05-07 20:53:48,848] Trial 53 pruned. \n",
            "[I 2024-05-07 20:56:14,974] Trial 54 pruned. \n",
            "[I 2024-05-07 20:57:29,173] Trial 55 pruned. \n",
            "[I 2024-05-07 20:58:14,315] Trial 56 pruned. \n",
            "[I 2024-05-07 21:02:10,119] Trial 57 pruned. \n",
            "[I 2024-05-07 21:07:04,306] Trial 58 pruned. \n",
            "[I 2024-05-07 21:11:20,962] Trial 59 pruned. \n",
            "[I 2024-05-07 21:11:45,334] Trial 60 pruned. \n",
            "[I 2024-05-07 21:12:50,835] Trial 61 pruned. \n",
            "[I 2024-05-07 21:13:56,922] Trial 62 pruned. \n",
            "[I 2024-05-07 21:25:09,639] Trial 63 pruned. \n",
            "[I 2024-05-07 21:38:35,151] Trial 64 finished with value: 0.5943999886512756 and parameters: {'num_conv_layers': 2, 'num_filter_0': 112.0, 'num_filter_1': 96.0, 'num_neurons': 390, 'drop_conv2': 0.3119338112008011, 'drop_fc1': 0.4293681221416072, 'optimizer': 'RMSprop', 'lr': 0.0003667203264637128}. Best is trial 28 with value: 0.5957000255584717.\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Estudio de optimización para una CNN de PyTorch con Optuna\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    # Usa cuda si está disponible para cálculos más rápidos\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # --- Parámetros ----------------------------------------------------------\n",
        "    n_epochs = 10                         # Número de épocas de entrenamiento\n",
        "    batch_size_train = 64                 # Tamaño del lote para datos de entrenamiento\n",
        "    batch_size_test = 1000                # Tamaño del lote para datos de prueba\n",
        "    number_of_trials = 100                # Número de pruebas de Optuna\n",
        "    limit_obs = True                      # Limitar el número de observaciones para cálculos más rápidos\n",
        "\n",
        "    # *** Nota: Para resultados más precisos, no limite las observaciones.\n",
        "    #           Sin embargo, si no se limitan, podría llevar mucho tiempo ejecutarlo.\n",
        "    #           Otra opción es limitar el número de épocas. ***\n",
        "\n",
        "    if limit_obs:                                          # Limitar el número de observaciones\n",
        "        number_of_train_examples = 500 * batch_size_train  # Máximas observaciones de entrenamiento\n",
        "        number_of_test_examples = 5 * batch_size_test      # Máximas observaciones de prueba\n",
        "    else:\n",
        "        number_of_train_examples = 60000                   # Máximas observaciones de entrenamiento\n",
        "        number_of_test_examples = 10000                    # Máximas observaciones de prueba\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    # Hacer que las ejecuciones sean repetibles\n",
        "    random_seed = 1\n",
        "    torch.backends.cudnn.enabled = False  # Deshabilitar el uso de algoritmos no deterministas de cuDNN\n",
        "    torch.manual_seed(random_seed)\n",
        "\n",
        "    # Crea el directorio 'files', si no existe, para guardar el conjunto de datos\n",
        "    directory_name = 'files'\n",
        "    if not os.path.exists(directory_name):\n",
        "        os.mkdir(directory_name)\n",
        "\n",
        "    # Descarga el conjunto de datos MNIST al directorio 'files' y normalízalo\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
        "                                   transform=torchvision.transforms.Compose([\n",
        "                                       torchvision.transforms.ToTensor(),\n",
        "                                       torchvision.transforms.Normalize((0.1307,), (0.3081,))])),\n",
        "        batch_size=batch_size_train, shuffle=True)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
        "                                   transform=torchvision.transforms.Compose([\n",
        "                                       torchvision.transforms.ToTensor(),\n",
        "                                       torchvision.transforms.Normalize((0.1307,), (0.3081,))])),\n",
        "        batch_size=batch_size_test, shuffle=True)\n",
        "\n",
        "    # Crea un estudio de Optuna para maximizar la precisión de prueba\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(objective, n_trials=number_of_trials)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoxkiNqAB9fS"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------------\n",
        "# Resultados\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "# Encuentra el número de pruebas podadas y completadas\n",
        "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
        "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
        "\n",
        "# Muestra las estadísticas del estudio\n",
        "print(\"\\nEstadísticas del estudio:\")\n",
        "print(\"  Número de pruebas finalizadas: \", len(study.trials))\n",
        "print(\"  Número de pruebas podadas: \", len(pruned_trials))\n",
        "print(\"  Número de pruebas completadas: \", len(complete_trials))\n",
        "\n",
        "trial = study.best_trial\n",
        "print(\"Mejor prueba:\")\n",
        "print(\"  Valor: \", trial.value)\n",
        "print(\"  Parámetros:\")\n",
        "for key, value in trial.params.items():\n",
        "    print(\"    {}: {}\".format(key, value))\n",
        "\n",
        "# Guarda los resultados en un archivo CSV\n",
        "df = study.trials_dataframe().drop(['datetime_start', 'datetime_complete', 'duration'], axis=1)  # Excluir columnas\n",
        "df = df.loc[df['state'] == 'COMPLETE']        # Mantener solo los resultados que no se podaron\n",
        "df = df.drop('state', axis=1)                 # Excluir columna de estado\n",
        "df = df.sort_values('value')                  # Ordenar según la precisión\n",
        "df.to_csv('optuna_results.csv', index=False)  # Guardar en archivo CSV\n",
        "\n",
        "# Muestra los resultados en un DataFrame\n",
        "print(\"\\nResultados generales (ordenados por precisión):\\n {}\".format(df))\n",
        "\n",
        "# Encuentra los hiperparámetros más importantes\n",
        "most_important_parameters = optuna.importance.get_param_importances(study, target=None)\n",
        "\n",
        "# Muestra los hiperparámetros más importantes\n",
        "print('\\nHiperparámetros más importantes:')\n",
        "for key, value in most_important_parameters.items():\n",
        "    print('  {}:{}{:.2f}%'.format(key, (15-len(key))*' ', value*100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJ0kudGGCENj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOiQmeeaAkDKC3RgUmzsAN3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}