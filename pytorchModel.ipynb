{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SantiagoGomezfpv/hyperparameter/blob/main/pytorchModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWgo41h7koFX"
      },
      "source": [
        "# Modelo Fully Connected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZn7ia9Mkm2I",
        "outputId": "009266ad-e778-4bdc-aae9-8325a2db3109"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 33858455.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 917722.46it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:49<00:00, 33398.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3823104.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "[1,   200] loss: 0.663\n",
            "[1,   400] loss: 0.318\n",
            "[1,   600] loss: 0.269\n",
            "[1,   800] loss: 0.242\n",
            "[2,   200] loss: 0.181\n",
            "[2,   400] loss: 0.166\n",
            "[2,   600] loss: 0.149\n",
            "[2,   800] loss: 0.155\n",
            "[3,   200] loss: 0.111\n",
            "[3,   400] loss: 0.121\n",
            "[3,   600] loss: 0.105\n",
            "[3,   800] loss: 0.107\n",
            "[4,   200] loss: 0.080\n",
            "[4,   400] loss: 0.083\n",
            "[4,   600] loss: 0.086\n",
            "[4,   800] loss: 0.086\n",
            "[5,   200] loss: 0.063\n",
            "[5,   400] loss: 0.066\n",
            "[5,   600] loss: 0.068\n",
            "[5,   800] loss: 0.062\n",
            "[6,   200] loss: 0.052\n",
            "[6,   400] loss: 0.053\n",
            "[6,   600] loss: 0.050\n",
            "[6,   800] loss: 0.054\n",
            "[7,   200] loss: 0.042\n",
            "[7,   400] loss: 0.044\n",
            "[7,   600] loss: 0.041\n",
            "[7,   800] loss: 0.042\n",
            "[8,   200] loss: 0.035\n",
            "[8,   400] loss: 0.031\n",
            "[8,   600] loss: 0.033\n",
            "[8,   800] loss: 0.036\n",
            "[9,   200] loss: 0.028\n",
            "[9,   400] loss: 0.026\n",
            "[9,   600] loss: 0.029\n",
            "[9,   800] loss: 0.035\n",
            "[10,   200] loss: 0.025\n",
            "[10,   400] loss: 0.023\n",
            "[10,   600] loss: 0.023\n",
            "[10,   800] loss: 0.025\n",
            "Precisión del modelo en el conjunto de test: 97 %\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Cargar y preparar datos MNIST\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Definir la red neuronal fully connected\n",
        "class FullyConnectedNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Crear modelo y optimizador\n",
        "model = FullyConnectedNet()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Entrenar el modelo\n",
        "for epoch in range(10):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:\n",
        "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 200))\n",
        "            running_loss = 0.0\n",
        "\n",
        "# Evaluar el modelo\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Precisión del modelo en el conjunto de test: %d %%' % (100 * correct / total))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHRdE_06k_wa"
      },
      "source": [
        "# Modelo CNN\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_w6MPqstlBKY",
        "outputId": "9e8a626a-45f8-4a1c-951c-5e4722590773"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,   100] loss: 0.693\n",
            "[1,   200] loss: 0.179\n",
            "[1,   300] loss: 0.120\n",
            "[1,   400] loss: 0.108\n",
            "[1,   500] loss: 0.094\n",
            "[1,   600] loss: 0.087\n",
            "[1,   700] loss: 0.071\n",
            "[1,   800] loss: 0.061\n",
            "[1,   900] loss: 0.062\n",
            "[2,   100] loss: 0.062\n",
            "[2,   200] loss: 0.047\n",
            "[2,   300] loss: 0.045\n",
            "[2,   400] loss: 0.057\n",
            "[2,   500] loss: 0.044\n",
            "[2,   600] loss: 0.049\n",
            "[2,   700] loss: 0.040\n",
            "[2,   800] loss: 0.040\n",
            "[2,   900] loss: 0.041\n",
            "[3,   100] loss: 0.032\n",
            "[3,   200] loss: 0.030\n",
            "[3,   300] loss: 0.028\n",
            "[3,   400] loss: 0.035\n",
            "[3,   500] loss: 0.030\n",
            "[3,   600] loss: 0.027\n",
            "[3,   700] loss: 0.029\n",
            "[3,   800] loss: 0.035\n",
            "[3,   900] loss: 0.031\n",
            "[4,   100] loss: 0.024\n",
            "[4,   200] loss: 0.021\n",
            "[4,   300] loss: 0.018\n",
            "[4,   400] loss: 0.024\n",
            "[4,   500] loss: 0.021\n",
            "[4,   600] loss: 0.027\n",
            "[4,   700] loss: 0.021\n",
            "[4,   800] loss: 0.033\n",
            "[4,   900] loss: 0.026\n",
            "[5,   100] loss: 0.017\n",
            "[5,   200] loss: 0.018\n",
            "[5,   300] loss: 0.015\n",
            "[5,   400] loss: 0.019\n",
            "[5,   500] loss: 0.025\n",
            "[5,   600] loss: 0.023\n",
            "[5,   700] loss: 0.012\n",
            "[5,   800] loss: 0.023\n",
            "[5,   900] loss: 0.020\n",
            "Finished Training\n",
            "Accuracy on the 10000 test images: 99 %\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Definir la arquitectura de la CNN\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.fc1 = nn.Linear(64*5*5, 128)  # Corregido el tamaño de entrada\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = torch.max_pool2d(x, 2)\n",
        "        x = self.conv2(x)\n",
        "        x = torch.relu(x)\n",
        "        x = torch.max_pool2d(x, 2)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Cargar los datos de MNIST\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "trainset, valset = torch.utils.data.random_split(trainset, [54000, 6000])\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Instanciar el modelo y definir la función de pérdida y el optimizador\n",
        "model = CNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 100))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# Evaluación del modelo\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_-EDpbmlxa4"
      },
      "source": [
        "# Optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPkwrCiQYM5M",
        "outputId": "5d0c08e2-e83f-40db-e9bb-fe17a707284f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.6.1-py3-none-any.whl (380 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.30)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.4)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.11.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
            "Installing collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.5 alembic-1.13.1 colorlog-6.8.2 optuna-3.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import optuna\n",
        "\n",
        "# Definir el dispositivo como CUDA si está disponible, de lo contrario usar CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Definición de la CNN\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, trial):\n",
        "        super(Net, self).__init__()\n",
        "        self.trial = trial  # Almacena el objeto trial como un atributo de la clase\n",
        "        # Optimizamos el número de capas convolucionales\n",
        "        n_layers = trial.suggest_int('n_layers', 1, 3)\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.pool = nn.MaxPool2d(2, 2)  # Definimos la capa de max pooling aquí\n",
        "        in_channels = 1\n",
        "        for i in range(n_layers):\n",
        "            out_channels = trial.suggest_int('n_filters_l{}'.format(i), 16, 128)\n",
        "            self.convs.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1))\n",
        "            in_channels = out_channels\n",
        "\n",
        "        # No definimos las capas completamente conectadas aquí porque necesitamos el tamaño de salida dinámico\n",
        "\n",
        "    def forward(self, x):\n",
        "        for conv in self.convs:\n",
        "            x = F.relu(conv(x))\n",
        "            x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        output_size = x.size(1)\n",
        "\n",
        "        # Creamos las capas completamente conectadas dinámicamente\n",
        "        if not hasattr(self, 'fc'):\n",
        "            self._create_fc_layers(output_size)\n",
        "\n",
        "        x = x.to(device)\n",
        "        for i, fc in enumerate(self.fc[:-1]):\n",
        "            x = F.relu(fc(x))\n",
        "            p = self.trial.suggest_float('dropout_l{}'.format(i), 0.2, 0.5)\n",
        "            x = F.dropout(x, p=p, training=self.training)\n",
        "        x = self.fc-1  # Corregir la llamada a la última capa lineal\n",
        "        return x\n",
        "\n",
        "    def _create_fc_layers(self, output_size):\n",
        "      # Creamos las capas completamente conectadas aquí\n",
        "      n_fc_layers = self.trial.suggest_int('n_fc_layers', 1, 3)\n",
        "      in_features = output_size\n",
        "      self.fc = nn.ModuleList()\n",
        "      for i in range(n_fc_layers):\n",
        "          out_features = self.trial.suggest_int('n_neurons_l{}'.format(i), 32, 1024)\n",
        "          self.fc.append(nn.Linear(in_features, out_features))\n",
        "          in_features = out_features\n",
        "      self.fc.append(nn.Linear(in_features, 10))  # 10 clases en MNIST\n",
        "      self.fc = self.fc.to(device)  # Mover la lista de módulos 'fc' al dispositivo correcto\n",
        "\n",
        "# Función objetivo para Optuna\n",
        "def objective(trial):\n",
        "    # Cargar datos de MNIST\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "    trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "    # Construir la red neuronal\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = Net(trial).to(device)\n",
        "\n",
        "    # Optimizamos el tipo de optimizador y la tasa de aprendizaje\n",
        "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop', 'SGD'])\n",
        "    lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
        "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
        "\n",
        "    # Entrenamiento de la red\n",
        "    for epoch in range(2):  # limitamos a 2 épocas para el ejemplo\n",
        "        model.train()\n",
        "        for data, target in trainloader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = F.cross_entropy(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Validación de la red\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in trainloader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Crear un estudio de Optuna y encontrar los mejores hiperparámetros\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=100)  # Ejecutar 100 pruebas para encontrar los mejores hiperparámetros\n",
        "\n",
        "# Imprimir los mejores hiperparámetros encontrados\n",
        "print(\"Mejores hiperparámetros:\", study.best_params)\n"
      ],
      "metadata": {
        "id": "F5NSTmuPXYrG",
        "outputId": "b4a46ec9-c69f-4ad8-e444-054cf08c2a70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-05-14 20:06:20,120] A new study created in memory with name: no-name-e3067aa5-8a17-450a-947e-ee0977b54a5a\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 167579127.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 15685056.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 81949608.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 8639695.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "[W 2024-05-14 20:06:22,276] Trial 0 failed with parameters: {'n_layers': 2, 'n_filters_l0': 114, 'n_filters_l1': 119, 'optimizer': 'SGD', 'lr': 0.05648034534415181, 'n_fc_layers': 3, 'n_neurons_l0': 783, 'n_neurons_l1': 379, 'n_neurons_l2': 443, 'dropout_l0': 0.45941427231179705, 'dropout_l1': 0.22808816411714922, 'dropout_l2': 0.26316712208006143} because of the following error: TypeError(\"unsupported operand type(s) for -: 'ModuleList' and 'int'\").\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"<ipython-input-2-107ee09b11e9>\", line 83, in objective\n",
            "    output = model(data)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"<ipython-input-2-107ee09b11e9>\", line 46, in forward\n",
            "    x = self.fc-1  # Corregir la llamada a la última capa lineal\n",
            "TypeError: unsupported operand type(s) for -: 'ModuleList' and 'int'\n",
            "[W 2024-05-14 20:06:22,278] Trial 0 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unsupported operand type(s) for -: 'ModuleList' and 'int'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-107ee09b11e9>\u001b[0m in \u001b[0;36m<cell line: 104>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;31m# Crear un estudio de Optuna y encontrar los mejores hiperparámetros\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Ejecutar 100 pruebas para encontrar los mejores hiperparámetros\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;31m# Imprimir los mejores hiperparámetros encontrados\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    449\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \"\"\"\n\u001b[0;32m--> 451\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    452\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     63\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     ):\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-107ee09b11e9>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-107ee09b11e9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dropout_l{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m  \u001b[0;31m# Corregir la llamada a la última capa lineal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'ModuleList' and 'int'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        },
        "id": "n8X09lvPW7C0",
        "outputId": "5a1fdf57-190b-4618-f912-faf15f4c49ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-05-14 19:23:20,114] A new study created in memory with name: no-name-5a2fbf7c-f6a9-4055-b140-dec490fbf18e\n",
            "[W 2024-05-14 19:23:31,308] Trial 0 failed with parameters: {'n_filters1': 64, 'n_filters2': 97, 'n_units': 200, 'lr': 2.131637717863943e-05} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"<ipython-input-20-2c34cf59e499>\", line 65, in objective\n",
            "    optimizer.step()                    # Actualización de los pesos\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\", line 385, in wrapper\n",
            "    out = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\", line 76, in _use_grad\n",
            "    ret = func(self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\", line 166, in step\n",
            "    adam(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\", line 316, in adam\n",
            "    func(params,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\", line 439, in _single_tensor_adam\n",
            "    denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)\n",
            "KeyboardInterrupt\n",
            "[W 2024-05-14 19:23:31,315] Trial 0 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-2c34cf59e499>\u001b[0m in \u001b[0;36m<cell line: 85>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;31m# Ejecutar la optimización de Optuna\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Número de pruebas finalizadas:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    449\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \"\"\"\n\u001b[0;32m--> 451\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    452\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     63\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     ):\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-2c34cf59e499>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Calcular la pérdida\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                     \u001b[0;31m# Retropropagación\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                    \u001b[0;31m# Actualización de los pesos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m                             )\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 state_steps)\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    167\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    317\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch                                        # Librería para aprendizaje profundo\n",
        "import torch.nn as nn                               # Contiene módulos para construir redes neuronales\n",
        "import torch.optim as optim                         # Contiene optimizadores para entrenar redes neuronales\n",
        "import torchvision                                  # Librería para datasets y transformaciones de visión computacional\n",
        "import torchvision.transforms as transforms         # Contiene funciones para preprocesar imágenes\n",
        "import optuna                                       # Librería para optimización hiperparámetros\n",
        "\n",
        "# Define la arquitectura de la CNN (Red Neuronal Convolucional)\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, trial):\n",
        "        super(CNN, self).__init__()\n",
        "        # Capa de convolución 1 con número de filtros variable\n",
        "        self.conv1 = nn.Conv2d(1, trial.suggest_int('n_filters1', 16, 64), kernel_size=3, stride=1)\n",
        "        # Capa de convolución 2 con número de filtros variable, dependiendo de la salida de la capa anterior\n",
        "        self.conv2 = nn.Conv2d(trial.suggest_int('n_filters1', 16, 64), trial.suggest_int('n_filters2', 32, 128), kernel_size=3, stride=1)\n",
        "        # Capa totalmente conectada (fully connected) con número de unidades variable\n",
        "        self.fc1 = nn.Linear(trial.suggest_int('n_filters2', 32, 128) * 5 * 5, trial.suggest_int('n_units', 64, 256))\n",
        "        # Capa de salida con 10 unidades para clasificar en las 10 clases de MNIST\n",
        "        self.fc2 = nn.Linear(trial.suggest_int('n_units', 64, 256), 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Operaciones en la propagación hacia adelante\n",
        "        x = self.conv1(x)\n",
        "        x = torch.relu(x)  # Función de activación ReLU (introducir no linealidades en los datos)\n",
        "        x = torch.max_pool2d(x, 2)  # Max pooling con tamaño de kernel 2x2\n",
        "        x = self.conv2(x)\n",
        "        x = torch.relu(x)\n",
        "        x = torch.max_pool2d(x, 2)\n",
        "        x = torch.flatten(x, 1)  # Aplanar la salida para la capa totalmente conectada\n",
        "        x = self.fc1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Cargar el conjunto de datos MNIST\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convertir las imágenes a tensores\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalizar los valores de píxeles\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "# Dividir el conjunto de datos de entrenamiento en entrenamiento y validación\n",
        "trainset, valset = torch.utils.data.random_split(trainset, [54000, 6000])\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Definir la función objetivo para Optuna\n",
        "def objective(trial):\n",
        "    model = CNN(trial)\n",
        "    criterion = nn.CrossEntropyLoss()  # Función de pérdida para problemas de clasificación\n",
        "    optimizer = optim.Adam(model.parameters(), lr=trial.suggest_float('lr', 1e-5, 1e-1, log=True))  # Optimizador Adam con tasa de aprendizaje variable\n",
        "    num_epochs = 5\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()               # Reiniciar los gradientes acumulados\n",
        "            outputs = model(inputs)             # Propagación hacia adelante\n",
        "            loss = criterion(outputs, labels)   # Calcular la pérdida\n",
        "            loss.backward()                     # Retropropagación\n",
        "            optimizer.step()                    # Actualización de los pesos\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "    # Evaluación del modelo en el conjunto de validación\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in valloader:\n",
        "            images, labels = data\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)  # Obtener las predicciones\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Ejecutar la optimización de Optuna\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "print('Número de pruebas finalizadas:', len(study.trials))\n",
        "print('Mejor prueba:')\n",
        "trial = study.best_trial\n",
        "print('  Valor: {}'.format(trial.value))\n",
        "print('  Parámetros: ')\n",
        "for key, value in trial.params.items():\n",
        "    print('    {}: {}'.format(key, value))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTn7sbd1YHUg"
      },
      "source": [
        "# Optimizador Optuna Elena\n",
        "\n",
        "[Optuna-optimization-for-PyTorch-CNN](https://github.com/elena-ecn/optuna-optimization-for-PyTorch-CNN/blob/main/optuna_optimization.py#L179)\n",
        "\n",
        "El conjunto de datos MNIST contiene **60.000** imágenes de **entrenamiento** y **10.000** imágenes de **prueba**.\n",
        "donde cada muestra es una imagen pequeña, cuadrada, en escala de grises de **28 × 28** píxeles de\n",
        "dígitos únicos escritos a mano entre 0 y 9.\n",
        "\n",
        "Los hiperparámetros de CNN elegidos para optimizar son:\n",
        "\n",
        "\n",
        "*   Tipo de optimizador\n",
        "*   Tasa de aprendizaje\n",
        "*   Valores de abandono\n",
        "*   Número de capas convolucionales\n",
        "*   Número de filtros de capas convolucionales\n",
        "*   Número de neuronas de capas completamente conectadas.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4qqlP5VYrGV"
      },
      "outputs": [],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2G7m3pOnq8ka"
      },
      "outputs": [],
      "source": [
        "import os                               # Importa el módulo os para interactuar con el sistema operativo\n",
        "import torch                            # Importa PyTorch\n",
        "import torchvision                      # Importa torchvision para trabajar con conjuntos de datos y modelos preentrenados\n",
        "import torch.nn as nn                   # Importa el módulo nn (neural network) de PyTorch para definir redes neuronales\n",
        "import torch.nn.functional as F         # Importa funciones de activación y pérdida de PyTorch\n",
        "import torch.optim as optim             # Importa optimizadores de PyTorch\n",
        "import optuna                           # Importa Optuna para la optimización de hiperparámetros\n",
        "from optuna.trial import TrialState     # Importa TrialState para el estado de las pruebas en Optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uP1oLxX3Yrsr"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, trial, num_conv_layers, num_filters, num_neurons, drop_conv2, drop_fc1):\n",
        "        \"\"\"\n",
        "        Parámetros:\n",
        "            - trial (optuna.trial._trial.Trial): Prueba de Optuna\n",
        "            - num_conv_layers (int):             Número de capas convolucionales\n",
        "            - num_filters (list):                Número de filtros de las capas convolucionales\n",
        "            - num_neurons (int):                 Número de neuronas de las capas totalmente conectadas\n",
        "            - drop_conv2 (float):                Ratio de dropout para la capa convolucional 2\n",
        "            - drop_fc1 (float):                  Ratio de dropout para FC1\n",
        "        \"\"\"\n",
        "        super(Net, self).__init__()                                                     # Inicializa la clase padre\n",
        "        in_size = 28                                                                    # Tamaño de la imagen de entrada (28 píxeles)\n",
        "        kernel_size = 3                                                                 # Tamaño del filtro de convolución\n",
        "\n",
        "        # Define las capas convolucionales\n",
        "        self.convs = nn.ModuleList([nn.Conv2d(1, num_filters[0], kernel_size=(3, 3))])  # Lista con las capas convolucionales\n",
        "        out_size = in_size - kernel_size + 1                                            # Tamaño del kernel de salida\n",
        "        out_size = int(out_size / 2)                                                    # Tamaño después del max-pooling\n",
        "        for i in range(1, num_conv_layers):\n",
        "            self.convs.append(nn.Conv2d(in_channels=num_filters[i-1], out_channels=num_filters[i], kernel_size=(3, 3)))\n",
        "            out_size = out_size - kernel_size + 1                                       # Tamaño del kernel de salida\n",
        "            out_size = int(out_size/2)                                                  # Tamaño después del max-pooling\n",
        "\n",
        "        self.conv2_drop = nn.Dropout2d(p=drop_conv2)                                    # Dropout para conv2\n",
        "        self.out_feature = num_filters[num_conv_layers-1] * out_size * out_size         # Tamaño de las características aplanadas\n",
        "        self.fc1 = nn.Linear(self.out_feature, num_neurons)                             # Capa totalmente conectada 1\n",
        "        self.fc2 = nn.Linear(num_neurons, 10)                                           # Capa totalmente conectada 2\n",
        "        self.p1 = drop_fc1                                                              # Ratio de dropout para FC1\n",
        "\n",
        "        # Inicializa los pesos con la inicialización de He\n",
        "        for i in range(1, num_conv_layers):\n",
        "            nn.init.kaiming_normal_(self.convs[i].weight, nonlinearity='relu')\n",
        "            if self.convs[i].bias is not None:\n",
        "                nn.init.constant_(self.convs[i].bias, 0)\n",
        "        nn.init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Propagación hacia adelante.\n",
        "\n",
        "        Parámetros:\n",
        "            - x (torch.Tensor): Tensor de entrada de tamaño [N,1,28,28]\n",
        "        Devuelve:\n",
        "            - (torch.Tensor): El tensor de salida después de la propagación hacia adelante [N,10]\n",
        "        \"\"\"\n",
        "        for i, conv_i in enumerate(self.convs):  # Para cada capa convolucional\n",
        "            if i == 2:  # Añade dropout si es la capa 2\n",
        "                x = F.relu(F.max_pool2d(self.conv2_drop(conv_i(x)), 2))  # Conv_i, dropout, max-pooling, RelU\n",
        "            else:\n",
        "                x = F.relu(F.max_pool2d(conv_i(x), 2))                   # Conv_i, max-pooling, RelU\n",
        "\n",
        "        x = x.view(-1, self.out_feature)                                 # Aplana el tensor\n",
        "        x = F.relu(self.fc1(x))                                          # FC1, RelU\n",
        "        x = F.dropout(x, p=self.p1, training=self.training)              # Aplica dropout después de FC1 solo durante el entrenamiento\n",
        "        x = self.fc2(x)                                                  # FC2\n",
        "\n",
        "        return F.log_softmax(x, dim=1)                                   # log(softmax(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OXbprGCgjcL"
      },
      "outputs": [],
      "source": [
        "def train(network, optimizer):\n",
        "    \"\"\"\n",
        "    Entrena el modelo.\n",
        "\n",
        "    Parámetros:\n",
        "        - red (__main__.Net):                      La CNN\n",
        "        - optimizador (torch.optim.<optimizador>): El optimizador para la CNN\n",
        "    \"\"\"\n",
        "    network.train()  # Establece el módulo en modo de entrenamiento (afecta solo ciertos módulos)\n",
        "    for batch_i, (data, target) in enumerate(train_loader):  # Para cada lote\n",
        "\n",
        "        # Limita los datos de entrenamiento para una computación más rápida\n",
        "        if batch_i * batch_size_train > number_of_train_examples:\n",
        "            break\n",
        "\n",
        "        optimizer.zero_grad()                                 # Borra los gradientes\n",
        "        output = network(data.to(device))                     # Propagación hacia adelante\n",
        "        loss = F.nll_loss(output, target.to(device))          # Calcula la pérdida (negative log likelihood: −log(y))\n",
        "        loss.backward()                                       # Calcula los gradientes\n",
        "        optimizer.step()                                      # Actualiza los pesos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vFp2NDzg1fN"
      },
      "outputs": [],
      "source": [
        "def test(network):\n",
        "    \"\"\"\n",
        "    Evalúa el modelo.\n",
        "\n",
        "    Parámetros:\n",
        "        - red (__main__.Net): La CNN\n",
        "\n",
        "    Devuelve:\n",
        "        - accuracy_test (torch.Tensor): La precisión de la prueba\n",
        "    \"\"\"\n",
        "    network.eval()         # Establece el módulo en modo de evaluación (afecta solo ciertos módulos)\n",
        "    correct = 0\n",
        "    with torch.no_grad():  # Deshabilita el cálculo de gradientes (cuando estás seguro de que no llamarás a Tensor.backward())\n",
        "        for batch_i, (data, target) in enumerate(test_loader):  # Para cada lote\n",
        "\n",
        "            # Limita los datos de prueba para una computación más rápida\n",
        "            if batch_i * batch_size_test > number_of_test_examples:\n",
        "                break\n",
        "\n",
        "            output = network(data.to(device))                                     # Propagación hacia adelante\n",
        "            pred = output.data.max(1, keepdim=True)[1]                            # Encuentra el valor máximo en cada fila, devuelve los índices de los valores máximos\n",
        "            correct += pred.eq(target.to(device).data.view_as(pred)).sum()        # Calcula las predicciones correctas\n",
        "\n",
        "    accuracy_test = correct / len(test_loader.dataset)\n",
        "\n",
        "    return accuracy_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glgDHgrnAqFT"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    \"\"\"Función objetivo a ser optimizada por Optuna.\n",
        "\n",
        "    Hiperparámetros elegidos para ser optimizados: optimizador, tasa de aprendizaje,\n",
        "    valores de dropout, número de capas convolucionales, número de filtros de las\n",
        "    capas convolucionales, número de neuronas de las capas totalmente conectadas.\n",
        "\n",
        "    Entradas:\n",
        "        - trial (optuna.trial._trial.Trial): Prueba de Optuna\n",
        "    Devuelve:\n",
        "        - accuracy (torch.Tensor): La precisión de la prueba. Parámetro a maximizar.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define el rango de valores a probar para los hiperparámetros\n",
        "    num_conv_layers = trial.suggest_int(\"num_conv_layers\", 2, 3)  # Número de capas convolucionales\n",
        "    num_filters = [int(trial.suggest_discrete_uniform(\"num_filter_\"+str(i), 16, 128, 16))\n",
        "                   for i in range(num_conv_layers)]               # Número de filtros para las capas convolucionales\n",
        "    num_neurons = trial.suggest_int(\"num_neurons\", 10, 400, 10)   # Número de neuronas de la capa FC1\n",
        "    drop_conv2 = trial.suggest_float(\"drop_conv2\", 0.2, 0.5)      # Dropout para la capa convolucional 2\n",
        "    drop_fc1 = trial.suggest_float(\"drop_fc1\", 0.2, 0.5)          # Dropout para la capa FC1\n",
        "\n",
        "    # Genera el modelo\n",
        "    model = Net(trial, num_conv_layers, num_filters, num_neurons, drop_conv2,  drop_fc1).to(device)\n",
        "\n",
        "    # Genera los optimizadores\n",
        "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])  # Optimizadores\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)                                 # Tasas de aprendizaje\n",
        "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
        "\n",
        "    # Entrenamiento del modelo\n",
        "    for epoch in range(n_epochs):\n",
        "        train(model, optimizer)  # Entrena el modelo\n",
        "        accuracy = test(model)   # Evalúa el modelo\n",
        "\n",
        "        # Para la poda (detiene la prueba temprano si no es prometedora)\n",
        "        trial.report(accuracy, epoch)\n",
        "        # Maneja la poda basada en el valor intermedio.\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHwEWLshA2Z5",
        "outputId": "dc5f809d-f22c-4a34-cd77-67930d2af962"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to /files/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:01<00:00, 5444301.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /files/MNIST/raw/train-images-idx3-ubyte.gz to /files/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to /files/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 160017.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /files/MNIST/raw/train-labels-idx1-ubyte.gz to /files/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to /files/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:04<00:00, 393172.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /files/MNIST/raw/t10k-images-idx3-ubyte.gz to /files/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to /files/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4292593.23it/s]\n",
            "[I 2024-05-07 17:09:26,506] A new study created in memory with name: no-name-123ea2a7-cc3a-482b-b37c-b19f3f9d1e36\n",
            "<ipython-input-7-67589e027cdb>:16: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
            "  num_filters = [int(trial.suggest_discrete_uniform(\"num_filter_\"+str(i), 16, 128, 16))\n",
            "<ipython-input-7-67589e027cdb>:18: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
            "  num_neurons = trial.suggest_int(\"num_neurons\", 10, 400, 10)  # Número de neuronas de la capa FC1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /files/MNIST/raw/t10k-labels-idx1-ubyte.gz to /files/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-05-07 17:27:21,348] Trial 0 finished with value: 0.48919999599456787 and parameters: {'num_conv_layers': 2, 'num_filter_0': 128.0, 'num_filter_1': 128.0, 'num_neurons': 340, 'drop_conv2': 0.21439508970072876, 'drop_fc1': 0.409350471900658, 'optimizer': 'SGD', 'lr': 4.735407160810673e-05}. Best is trial 0 with value: 0.48919999599456787.\n",
            "[I 2024-05-07 17:37:19,810] Trial 1 finished with value: 0.595300018787384 and parameters: {'num_conv_layers': 2, 'num_filter_0': 112.0, 'num_filter_1': 48.0, 'num_neurons': 360, 'drop_conv2': 0.3637459814288115, 'drop_fc1': 0.36470795939010936, 'optimizer': 'Adam', 'lr': 0.0008584929846386581}. Best is trial 1 with value: 0.595300018787384.\n",
            "[I 2024-05-07 17:45:10,827] Trial 2 finished with value: 0.5598999857902527 and parameters: {'num_conv_layers': 3, 'num_filter_0': 80.0, 'num_filter_1': 48.0, 'num_filter_2': 112.0, 'num_neurons': 380, 'drop_conv2': 0.29614793864676914, 'drop_fc1': 0.3769548775900181, 'optimizer': 'Adam', 'lr': 1.420577204735963e-05}. Best is trial 1 with value: 0.595300018787384.\n",
            "[I 2024-05-07 17:47:49,748] Trial 3 finished with value: 0.5914999842643738 and parameters: {'num_conv_layers': 2, 'num_filter_0': 16.0, 'num_filter_1': 32.0, 'num_neurons': 290, 'drop_conv2': 0.36518923835495387, 'drop_fc1': 0.2534757834127089, 'optimizer': 'RMSprop', 'lr': 0.006895583476179975}. Best is trial 1 with value: 0.595300018787384.\n",
            "[I 2024-05-07 17:59:50,412] Trial 4 finished with value: 0.5932000279426575 and parameters: {'num_conv_layers': 2, 'num_filter_0': 128.0, 'num_filter_1': 64.0, 'num_neurons': 320, 'drop_conv2': 0.4330365829275203, 'drop_fc1': 0.27775098798956704, 'optimizer': 'RMSprop', 'lr': 7.32614777609411e-05}. Best is trial 1 with value: 0.595300018787384.\n",
            "[I 2024-05-07 18:00:24,749] Trial 5 pruned. \n",
            "[I 2024-05-07 18:01:31,341] Trial 6 pruned. \n",
            "[I 2024-05-07 18:02:49,646] Trial 7 pruned. \n",
            "[I 2024-05-07 18:03:28,431] Trial 8 pruned. \n",
            "[I 2024-05-07 18:04:19,647] Trial 9 pruned. \n",
            "[I 2024-05-07 18:16:06,705] Trial 10 finished with value: 0.5943999886512756 and parameters: {'num_conv_layers': 2, 'num_filter_0': 96.0, 'num_filter_1': 96.0, 'num_neurons': 400, 'drop_conv2': 0.4778284508913528, 'drop_fc1': 0.21227017964763953, 'optimizer': 'Adam', 'lr': 0.0006682581848924286}. Best is trial 1 with value: 0.595300018787384.\n",
            "[I 2024-05-07 18:27:55,017] Trial 11 finished with value: 0.5954999923706055 and parameters: {'num_conv_layers': 2, 'num_filter_0': 96.0, 'num_filter_1': 96.0, 'num_neurons': 400, 'drop_conv2': 0.49920681816314133, 'drop_fc1': 0.2318516934662937, 'optimizer': 'Adam', 'lr': 0.0006600398024524423}. Best is trial 11 with value: 0.5954999923706055.\n",
            "[I 2024-05-07 18:40:10,179] Trial 12 finished with value: 0.5946999788284302 and parameters: {'num_conv_layers': 2, 'num_filter_0': 112.0, 'num_filter_1': 80.0, 'num_neurons': 400, 'drop_conv2': 0.42841445462391153, 'drop_fc1': 0.22108540758700376, 'optimizer': 'Adam', 'lr': 0.0005664536093733146}. Best is trial 11 with value: 0.5954999923706055.\n",
            "[I 2024-05-07 18:46:34,514] Trial 13 finished with value: 0.5939000248908997 and parameters: {'num_conv_layers': 2, 'num_filter_0': 48.0, 'num_filter_1': 80.0, 'num_neurons': 240, 'drop_conv2': 0.4898321347980872, 'drop_fc1': 0.323228450980815, 'optimizer': 'Adam', 'lr': 0.0020965370567537197}. Best is trial 11 with value: 0.5954999923706055.\n",
            "[I 2024-05-07 18:47:42,558] Trial 14 pruned. \n",
            "[I 2024-05-07 18:58:34,476] Trial 15 finished with value: 0.5946000218391418 and parameters: {'num_conv_layers': 2, 'num_filter_0': 112.0, 'num_filter_1': 64.0, 'num_neurons': 340, 'drop_conv2': 0.2777523005114743, 'drop_fc1': 0.44620752002478015, 'optimizer': 'RMSprop', 'lr': 0.0001984886478101404}. Best is trial 11 with value: 0.5954999923706055.\n",
            "[I 2024-05-07 18:59:04,261] Trial 16 pruned. \n",
            "[I 2024-05-07 19:00:01,039] Trial 17 pruned. \n",
            "[I 2024-05-07 19:01:09,483] Trial 18 pruned. \n",
            "[I 2024-05-07 19:01:48,725] Trial 19 pruned. \n",
            "[I 2024-05-07 19:03:00,832] Trial 20 pruned. \n",
            "[I 2024-05-07 19:15:20,294] Trial 21 finished with value: 0.5950000286102295 and parameters: {'num_conv_layers': 2, 'num_filter_0': 112.0, 'num_filter_1': 80.0, 'num_neurons': 400, 'drop_conv2': 0.4482861961736407, 'drop_fc1': 0.22034278775028793, 'optimizer': 'Adam', 'lr': 0.0006796373440228386}. Best is trial 11 with value: 0.5954999923706055.\n",
            "[I 2024-05-07 19:21:12,486] Trial 22 pruned. \n",
            "[I 2024-05-07 19:22:09,946] Trial 23 pruned. \n",
            "[I 2024-05-07 19:23:52,992] Trial 24 pruned. \n",
            "[I 2024-05-07 19:29:14,211] Trial 25 pruned. \n",
            "[I 2024-05-07 19:31:07,542] Trial 26 pruned. \n",
            "[I 2024-05-07 19:31:52,399] Trial 27 pruned. \n",
            "[I 2024-05-07 19:45:00,214] Trial 28 finished with value: 0.5957000255584717 and parameters: {'num_conv_layers': 2, 'num_filter_0': 128.0, 'num_filter_1': 64.0, 'num_neurons': 400, 'drop_conv2': 0.4142984520518183, 'drop_fc1': 0.4117088919540175, 'optimizer': 'RMSprop', 'lr': 0.0002532809844335816}. Best is trial 28 with value: 0.5957000255584717.\n",
            "[I 2024-05-07 19:46:19,267] Trial 29 pruned. \n",
            "[I 2024-05-07 19:47:42,100] Trial 30 pruned. \n",
            "[I 2024-05-07 19:48:48,042] Trial 31 pruned. \n",
            "[I 2024-05-07 19:51:56,461] Trial 32 pruned. \n",
            "[I 2024-05-07 19:52:56,625] Trial 33 pruned. \n",
            "[I 2024-05-07 19:56:15,183] Trial 34 pruned. \n",
            "[I 2024-05-07 19:57:07,500] Trial 35 pruned. \n",
            "[I 2024-05-07 19:58:26,585] Trial 36 pruned. \n",
            "[I 2024-05-07 19:58:42,372] Trial 37 pruned. \n",
            "[I 2024-05-07 19:59:39,099] Trial 38 pruned. \n",
            "[I 2024-05-07 20:00:14,433] Trial 39 pruned. \n",
            "[I 2024-05-07 20:01:15,708] Trial 40 pruned. \n",
            "[I 2024-05-07 20:06:11,890] Trial 41 pruned. \n",
            "[I 2024-05-07 20:17:10,228] Trial 42 finished with value: 0.5946000218391418 and parameters: {'num_conv_layers': 2, 'num_filter_0': 112.0, 'num_filter_1': 64.0, 'num_neurons': 240, 'drop_conv2': 0.42473031112124926, 'drop_fc1': 0.24108877799453443, 'optimizer': 'Adam', 'lr': 0.00052638566713028}. Best is trial 28 with value: 0.5957000255584717.\n",
            "[I 2024-05-07 20:18:16,055] Trial 43 pruned. \n",
            "[I 2024-05-07 20:19:46,547] Trial 44 pruned. \n",
            "[I 2024-05-07 20:30:33,986] Trial 45 finished with value: 0.5931000113487244 and parameters: {'num_conv_layers': 2, 'num_filter_0': 112.0, 'num_filter_1': 64.0, 'num_neurons': 120, 'drop_conv2': 0.29714847361429947, 'drop_fc1': 0.426152512015162, 'optimizer': 'Adam', 'lr': 0.0024244686667653187}. Best is trial 28 with value: 0.5957000255584717.\n",
            "[I 2024-05-07 20:31:47,778] Trial 46 pruned. \n",
            "[I 2024-05-07 20:36:30,550] Trial 47 pruned. \n",
            "[I 2024-05-07 20:37:39,039] Trial 48 pruned. \n",
            "[I 2024-05-07 20:38:45,981] Trial 49 pruned. \n",
            "[I 2024-05-07 20:39:47,253] Trial 50 pruned. \n",
            "[I 2024-05-07 20:50:45,122] Trial 51 finished with value: 0.5946999788284302 and parameters: {'num_conv_layers': 2, 'num_filter_0': 112.0, 'num_filter_1': 64.0, 'num_neurons': 340, 'drop_conv2': 0.2653288731453715, 'drop_fc1': 0.45968073418590116, 'optimizer': 'RMSprop', 'lr': 0.0005518808437566115}. Best is trial 28 with value: 0.5957000255584717.\n",
            "[I 2024-05-07 20:52:44,770] Trial 52 pruned. \n",
            "[I 2024-05-07 20:53:48,848] Trial 53 pruned. \n",
            "[I 2024-05-07 20:56:14,974] Trial 54 pruned. \n",
            "[I 2024-05-07 20:57:29,173] Trial 55 pruned. \n",
            "[I 2024-05-07 20:58:14,315] Trial 56 pruned. \n",
            "[I 2024-05-07 21:02:10,119] Trial 57 pruned. \n",
            "[I 2024-05-07 21:07:04,306] Trial 58 pruned. \n",
            "[I 2024-05-07 21:11:20,962] Trial 59 pruned. \n",
            "[I 2024-05-07 21:11:45,334] Trial 60 pruned. \n",
            "[I 2024-05-07 21:12:50,835] Trial 61 pruned. \n",
            "[I 2024-05-07 21:13:56,922] Trial 62 pruned. \n",
            "[I 2024-05-07 21:25:09,639] Trial 63 pruned. \n",
            "[I 2024-05-07 21:38:35,151] Trial 64 finished with value: 0.5943999886512756 and parameters: {'num_conv_layers': 2, 'num_filter_0': 112.0, 'num_filter_1': 96.0, 'num_neurons': 390, 'drop_conv2': 0.3119338112008011, 'drop_fc1': 0.4293681221416072, 'optimizer': 'RMSprop', 'lr': 0.0003667203264637128}. Best is trial 28 with value: 0.5957000255584717.\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Estudio de optimización para una CNN de PyTorch con Optuna\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    # Usa cuda si está disponible para cálculos más rápidos\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # --- Parámetros ----------------------------------------------------------\n",
        "    n_epochs = 10                         # Número de épocas de entrenamiento\n",
        "    batch_size_train = 64                 # Tamaño del lote para datos de entrenamiento\n",
        "    batch_size_test = 1000                # Tamaño del lote para datos de prueba\n",
        "    number_of_trials = 100                # Número de pruebas de Optuna\n",
        "    limit_obs = True                      # Limitar el número de observaciones para cálculos más rápidos\n",
        "\n",
        "    # *** Nota: Para resultados más precisos, no limite las observaciones.\n",
        "    #           Sin embargo, si no se limitan, podría llevar mucho tiempo ejecutarlo.\n",
        "    #           Otra opción es limitar el número de épocas. ***\n",
        "\n",
        "    if limit_obs:                                          # Limitar el número de observaciones\n",
        "        number_of_train_examples = 500 * batch_size_train  # Máximas observaciones de entrenamiento\n",
        "        number_of_test_examples = 5 * batch_size_test      # Máximas observaciones de prueba\n",
        "    else:\n",
        "        number_of_train_examples = 60000                   # Máximas observaciones de entrenamiento\n",
        "        number_of_test_examples = 10000                    # Máximas observaciones de prueba\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    # Hacer que las ejecuciones sean repetibles\n",
        "    random_seed = 1\n",
        "    torch.backends.cudnn.enabled = False  # Deshabilitar el uso de algoritmos no deterministas de cuDNN\n",
        "    torch.manual_seed(random_seed)\n",
        "\n",
        "    # Crea el directorio 'files', si no existe, para guardar el conjunto de datos\n",
        "    directory_name = 'files'\n",
        "    if not os.path.exists(directory_name):\n",
        "        os.mkdir(directory_name)\n",
        "\n",
        "    # Descarga el conjunto de datos MNIST al directorio 'files' y normalízalo\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
        "                                   transform=torchvision.transforms.Compose([\n",
        "                                       torchvision.transforms.ToTensor(),\n",
        "                                       torchvision.transforms.Normalize((0.1307,), (0.3081,))])),\n",
        "        batch_size=batch_size_train, shuffle=True)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
        "                                   transform=torchvision.transforms.Compose([\n",
        "                                       torchvision.transforms.ToTensor(),\n",
        "                                       torchvision.transforms.Normalize((0.1307,), (0.3081,))])),\n",
        "        batch_size=batch_size_test, shuffle=True)\n",
        "\n",
        "    # Crea un estudio de Optuna para maximizar la precisión de prueba\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(objective, n_trials=number_of_trials)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoxkiNqAB9fS"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------------\n",
        "# Resultados\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "# Encuentra el número de pruebas podadas y completadas\n",
        "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
        "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
        "\n",
        "# Muestra las estadísticas del estudio\n",
        "print(\"\\nEstadísticas del estudio:\")\n",
        "print(\"  Número de pruebas finalizadas: \", len(study.trials))\n",
        "print(\"  Número de pruebas podadas: \", len(pruned_trials))\n",
        "print(\"  Número de pruebas completadas: \", len(complete_trials))\n",
        "\n",
        "trial = study.best_trial\n",
        "print(\"Mejor prueba:\")\n",
        "print(\"  Valor: \", trial.value)\n",
        "print(\"  Parámetros:\")\n",
        "for key, value in trial.params.items():\n",
        "    print(\"    {}: {}\".format(key, value))\n",
        "\n",
        "# Guarda los resultados en un archivo CSV\n",
        "df = study.trials_dataframe().drop(['datetime_start', 'datetime_complete', 'duration'], axis=1)  # Excluir columnas\n",
        "df = df.loc[df['state'] == 'COMPLETE']        # Mantener solo los resultados que no se podaron\n",
        "df = df.drop('state', axis=1)                 # Excluir columna de estado\n",
        "df = df.sort_values('value')                  # Ordenar según la precisión\n",
        "df.to_csv('optuna_results.csv', index=False)  # Guardar en archivo CSV\n",
        "\n",
        "# Muestra los resultados en un DataFrame\n",
        "print(\"\\nResultados generales (ordenados por precisión):\\n {}\".format(df))\n",
        "\n",
        "# Encuentra los hiperparámetros más importantes\n",
        "most_important_parameters = optuna.importance.get_param_importances(study, target=None)\n",
        "\n",
        "# Muestra los hiperparámetros más importantes\n",
        "print('\\nHiperparámetros más importantes:')\n",
        "for key, value in most_important_parameters.items():\n",
        "    print('  {}:{}{:.2f}%'.format(key, (15-len(key))*' ', value*100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJ0kudGGCENj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyG (PyTorch Geometric)\n",
        "\n",
        "[Documentation](https://pytorch-geometric.readthedocs.io/en/latest/get_started/introduction.html#learning-methods-on-graphs)"
      ],
      "metadata": {
        "id": "lU7aQiLd7E-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar: https://gist.github.com/ameya98/b193856171d11d37ada46458f60e73e7?permalink_comment_id=3674777\n",
        "\n",
        "import torch\n",
        "\n",
        "def format_pytorch_version(version):\n",
        "  return version.split('+')[0]\n",
        "\n",
        "TORCH_version = torch.__version__\n",
        "TORCH = format_pytorch_version(TORCH_version)\n",
        "\n",
        "def format_cuda_version(version):\n",
        "  return 'cu' + version.replace('.', '')\n",
        "\n",
        "CUDA_version = torch.version.cuda\n",
        "CUDA = format_cuda_version(CUDA_version)\n",
        "\n",
        "!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc2K0gRf7GIT",
        "outputId": "e8ecac21-5ebe-4031-f095-225f8eb1f80b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.2.1+cu121.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.2.0%2Bcu121/torch_scatter-2.1.2%2Bpt22cu121-cp310-cp310-linux_x86_64.whl (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt22cu121\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.2.1+cu121.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.2.0%2Bcu121/torch_sparse-0.6.18%2Bpt22cu121-cp310-cp310-linux_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.25.2)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt22cu121\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.2.1+cu121.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.2.0%2Bcu121/torch_cluster-1.6.3%2Bpt22cu121-cp310-cp310-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-cluster) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-cluster) (1.25.2)\n",
            "Installing collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.3+pt22cu121\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.2.1+cu121.html\n",
            "Collecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.2.0%2Bcu121/torch_spline_conv-1.2.2%2Bpt22cu121-cp310-cp310-linux_x86_64.whl (943 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m943.4/943.4 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.2+pt22cu121\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.9.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.5.0)\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.datasets import Planetoid\n",
        "\n",
        "dataset = Planetoid(root='/tmp/Cora', name='Cora')"
      ],
      "metadata": {
        "id": "UwwQoQ9s8BNx",
        "outputId": "5c60c38c-c0bf-48bf-802d-f993d65b9478",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Definición de la primera capa de convolución GCN.\n",
        "        # dataset.num_node_features es el número de características en cada nodo del grafo.\n",
        "        # La primera capa convolucional transforma estas características en un espacio de características de tamaño 16.\n",
        "        self.conv1 = GCNConv(dataset.num_node_features, 16)\n",
        "        # Definición de la segunda capa de convolución GCN.\n",
        "        # La entrada son 16 características producidas por la capa anterior,\n",
        "        # y la salida es un vector de características de tamaño igual al número de clases en el dataset.\n",
        "        self.conv2 = GCNConv(16, dataset.num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        # Se obtienen los datos de entrada: x son las características de los nodos y edge_index es la estructura del grafo.\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        # Paso de la primera capa GCN.\n",
        "        x = self.conv1(x, edge_index)\n",
        "        # Aplicación de la función de activación ReLU.\n",
        "        x = F.relu(x)\n",
        "        # Aplicación de dropout para regularización.\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        # Paso de la segunda capa GCN.\n",
        "        x = self.conv2(x, edge_index)\n",
        "\n",
        "        # Se aplica la función de activación log_softmax para obtener la salida del modelo.\n",
        "        # Esto proporciona la probabilidad de que cada nodo pertenezca a cada clase.\n",
        "        return F.log_softmax(x, dim=1)\n"
      ],
      "metadata": {
        "id": "tENXJgu8841w"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecciona el dispositivo de cómputo (GPU si está disponible, de lo contrario, CPU).\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Instancia el modelo GCN y lo mueve al dispositivo seleccionado.\n",
        "model = GCN().to(device)\n",
        "# Mueve los datos de entrada al dispositivo seleccionado.\n",
        "data = dataset[0].to(device)\n",
        "# Define el optimizador Adam para actualizar los parámetros del modelo durante el entrenamiento.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "\n",
        "# Indica que el modelo está en modo de entrenamiento.\n",
        "model.train()\n",
        "# Ciclo de entrenamiento que itera sobre un número fijo de épocas (en este caso, 200).\n",
        "for epoch in range(200):\n",
        "    # Reinicia los gradientes del optimizador.\n",
        "    optimizer.zero_grad()\n",
        "    # Propaga los datos de entrada a través del modelo para obtener las predicciones.\n",
        "    out = model(data)\n",
        "    # Calcula la función de pérdida NLL (negative log likelihood) para los nodos de entrenamiento.\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "    # Retropropaga el error para calcular los gradientes.\n",
        "    loss.backward()\n",
        "    # Actualiza los parámetros del modelo utilizando el optimizador.\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "id": "3O1mDWKG9CVY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Indica que el modelo está en modo de evaluación. Esto desactiva capas como Dropout que se usan durante el entrenamiento.\n",
        "model.eval()\n",
        "# Realiza predicciones en los datos de prueba y obtiene la clase predicha para cada nodo.\n",
        "pred = model(data).argmax(dim=1)\n",
        "# Calcula el número de predicciones correctas comparando las predicciones con las etiquetas verdaderas de los nodos de prueba.\n",
        "correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
        "# Calcula la precisión dividiendo el número de predicciones correctas entre el número total de nodos de prueba.\n",
        "acc = int(correct) / int(data.test_mask.sum())\n",
        "# Imprime la precisión.\n",
        "print(f'Accuracy: {acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3afacdQt9EAo",
        "outputId": "fc94f0a5-5a74-45cc-b1d7-9657f3231ff4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7960\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B85EHcYqTX3b"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVOgxjfGOEY526nH4plC7/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}